---
title: "What is Flumes Memory?"
description: "Give your AI the memory it deserves in one API call."
---

# Flumes Memory

Flumes Memory is a **managed memory layer** for large-language-model (LLM) applications. With a single HTTP call you can store, semantically index, and retrieve any chunk of text or JSON — **no vector-database plumbing required**.

<Columns cols={2}>
  <Info>
  • **Zero-ops** – Forget about provisioning vector DBs, tuning indexes, or batch jobs.<br/>
  • **Cost-aware** – We only charge for what you store, no per-query fees.<br/>
  • **Opinionated APIs** – Designed around the way agents actually think: *memories in, memories out*.
  </Info>
  <Check>
  Works out-of-the-box for chatbots, autonomous agents, and RAG pipelines.
  </Check>
</Columns>

## Why Flumes over traditional vector DBs?

|             | Flumes Memory | Zep / Mem0 | Pinecone / Weaviate |
|-------------|---------------|------------|---------------------|
| Plug-and-play REST API | ✅ | ✅ | ❌ requires SDK |
| Message-level schema | ✅ | ✅ | ❌ |
| Built-in metadata & auth scopes | ✅ | ✅ | ❌ |
| Cost-aware storage | ✅ | ❔ | ❌ |

<Note>
Need a deeper dive? Jump to the [Memory Model](/guides/overview/memory-model) or head straight to the [Quickstart](/guides/getting-started/quickstart).
</Note>
